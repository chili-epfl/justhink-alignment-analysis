{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reads transcripts, extracts routines, and exports routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from read_utils import read_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_sort(l):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
    "    return sorted(l, key = alphanum_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ../processed_data/dialign_inputs\n",
      "Created ../processed_data/dialign_outputs\n",
      "Created ../processed_data/routines\n"
     ]
    }
   ],
   "source": [
    "# Inputs.\n",
    "data_dir = pl.Path('../data')\n",
    "transcripts_dir = data_dir.joinpath('transcripts')\n",
    "\n",
    "dialign_dir = pl.Path('dialign-1.0')\n",
    "dialign_jar_file = dialign_dir.joinpath('dialign.jar')\n",
    "\n",
    "output_dir = pl.Path('../outputs')\n",
    "interm_dir = output_dir.joinpath('intermediate')\n",
    "task_features_file = interm_dir.joinpath(\n",
    "    'log_features/justhink19_log_features_task_level.csv')\n",
    "\n",
    "# Outputs.\n",
    "processed_data_dir = pl.Path('../processed_data')\n",
    "dialign_inputs_dir = processed_data_dir.joinpath('dialign_inputs')\n",
    "dialign_outputs_dir = processed_data_dir.joinpath('dialign_outputs')\n",
    "routines_dir = processed_data_dir.joinpath('routines')\n",
    "\n",
    "dirs = [\n",
    "    dialign_inputs_dir, dialign_outputs_dir,  # dialign_transcripts_dir,\n",
    "    routines_dir,\n",
    "]\n",
    "for d in dirs:\n",
    "    if not d.exists():\n",
    "        d.mkdir()\n",
    "        print('Created {}'.format(d))\n",
    "\n",
    "\n",
    "synthesis_dep_file = dialign_outputs_dir.joinpath(\n",
    "    'metrics-speaker-dependent.tsv')\n",
    "synthesis_indep_file = dialign_outputs_dir.joinpath(\n",
    "    'metrics-speaker-dependent.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define task-specific referents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['basel', 'bern', 'davos', 'gallen', 'interlaken', 'luzern', 'montreux', 'neuchatel', 'zermatt', 'zurich']\n"
     ]
    }
   ],
   "source": [
    "node_words = {\n",
    "    'basel',\n",
    "    'luzern',\n",
    "    'zurich',\n",
    "    'bern',\n",
    "    'zermatt',\n",
    "    'interlaken',\n",
    "    'montreux',\n",
    "    'neuchatel',\n",
    "    'gallen',\n",
    "    'davos',\n",
    "}\n",
    "\n",
    "task_words = node_words\n",
    "\n",
    "print(len(task_words), sorted(task_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading transcript files from ../data/transcripts.\n",
      "transcript 10 files found.\n",
      "File justhink19_transcript_07 belongs to team  7\n",
      "File justhink19_transcript_08 belongs to team  8\n",
      "File justhink19_transcript_09 belongs to team  9\n",
      "File justhink19_transcript_10 belongs to team 10\n",
      "File justhink19_transcript_11 belongs to team 11\n",
      "File justhink19_transcript_17 belongs to team 17\n",
      "File justhink19_transcript_18 belongs to team 18\n",
      "File justhink19_transcript_20 belongs to team 20\n",
      "File justhink19_transcript_28 belongs to team 28\n",
      "File justhink19_transcript_47 belongs to team 47\n",
      "Transcript of  7 has  639 utterances\n",
      "Transcript of  8 has  669 utterances\n",
      "Transcript of  9 has  810 utterances\n",
      "Transcript of 10 has  469 utterances\n",
      "Transcript of 11 has  567 utterances\n",
      "Transcript of 17 has  325 utterances\n",
      "Transcript of 18 has  359 utterances\n",
      "Transcript of 20 has  507 utterances\n",
      "Transcript of 28 has  348 utterances\n",
      "Transcript of 47 has  396 utterances\n"
     ]
    }
   ],
   "source": [
    "transcript_dfs = read_tables(transcripts_dir, form='transcript')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine task and transcript durations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute speaking durations from transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{7: 1573.338,\n",
       " 8: 1570.4229999999998,\n",
       " 9: 2160.63,\n",
       " 10: 1384.955,\n",
       " 11: 1580.5079999999998,\n",
       " 17: 1075.073,\n",
       " 18: 1804.0870000000002,\n",
       " 20: 1155.259,\n",
       " 28: 669.247,\n",
       " 47: 1178.847}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_times = dict()\n",
    "\n",
    "for team_no, df in transcript_dfs.items():       \n",
    "    dff = df[df['utterance'] == '(omitted)']\n",
    "    if len(dff) > 0:\n",
    "        end_time = dff['start'].min()\n",
    "    else:\n",
    "        end_time = df.iloc[-1]['end']\n",
    "                \n",
    "    end_times[team_no] = end_time\n",
    "\n",
    "end_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the total transcribed duration in hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9312130555555553"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [td / 60 / 60 for td in end_times.values()]\n",
    "sum(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice the transcripts by their inferred duration. \n",
    "There is sometimes more talk after the task ends, some of which was also transcribed, we omit that.\n",
    "This is specifically when the team fails i.e. time is up, and we intervene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for team_no in transcript_dfs:\n",
    "    df = transcript_dfs[team_no]\n",
    "    df = df[df.end <= end_times[team_no]]\n",
    "    transcript_dfs[team_no] = df\n",
    "    \n",
    "# # A quick check.\n",
    "# transcript_dfs[7].tail(), end_times[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate inputs for dialign to extract routines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a tokenizer. \n",
    "Create a Tokenizer with the default settings for English, including punctuation rules and exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a tokeniser method for dialign (as per dialign input format). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_utterances(df, tokenizer):\n",
    "    df = df.copy()\n",
    "    texts = list()\n",
    "    for u in df['utterance']:\n",
    "        tokens = tokenizer(u)\n",
    "        text = ' '.join([t.text for t in tokens])\n",
    "        texts.append(text)\n",
    "\n",
    "    df['utterance'] = texts\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an exporter for dialign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_for_dialign(df, file):\n",
    "    with open(str(file), 'w') as f:\n",
    "        for i, row in df.iterrows():\n",
    "            print('{}\\t{}'.format(row['interlocutor'], row['utterance']),\n",
    "                  file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the transcripts for dialign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting the transcripts in dialign input format...\n",
      "Written task  7 to ../processed_data/dialign_inputs/justhink19_dialogue_07.tsv\n",
      "Written task  8 to ../processed_data/dialign_inputs/justhink19_dialogue_08.tsv\n",
      "Written task  9 to ../processed_data/dialign_inputs/justhink19_dialogue_09.tsv\n",
      "Written task 10 to ../processed_data/dialign_inputs/justhink19_dialogue_10.tsv\n",
      "Written task 11 to ../processed_data/dialign_inputs/justhink19_dialogue_11.tsv\n",
      "Written task 17 to ../processed_data/dialign_inputs/justhink19_dialogue_17.tsv\n",
      "Written task 18 to ../processed_data/dialign_inputs/justhink19_dialogue_18.tsv\n",
      "Written task 20 to ../processed_data/dialign_inputs/justhink19_dialogue_20.tsv\n",
      "Written task 28 to ../processed_data/dialign_inputs/justhink19_dialogue_28.tsv\n",
      "Written task 47 to ../processed_data/dialign_inputs/justhink19_dialogue_47.tsv\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Exporting the transcripts in dialign input format...')\n",
    "\n",
    "for task_index, df in transcript_dfs.items():\n",
    "    # Construct filename.\n",
    "    file = 'justhink19_dialogue_{:02d}.tsv'.format(task_index)\n",
    "    file = dialign_inputs_dir.joinpath(file)\n",
    "    \n",
    "    # Slice for interlocutors A and B only.\n",
    "    df = df[df['interlocutor'].isin(['A', 'B'])]\n",
    "\n",
    "    # Tokenise.\n",
    "    df = tokenise_utterances(df, tokenizer)\n",
    "    \n",
    "    # Export table to file.\n",
    "    export_for_dialign(df, file)\n",
    "\n",
    "    print('Written task {:2d} to {}'.format(task_index, file))\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the transcripts in full, compliant with dialign inputs/indices.\n",
    "e.g. for aligning with actions.\n",
    "The tables contain additional information like start and end times of the utterances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run dialign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java -jar /home/utku/playground/justhink-dialogue-and-actions-corpus/tools/dialign-1.0/dialign.jar -i /home/utku/playground/justhink-dialogue-and-actions-corpus/processed_data/dialign_inputs -o /home/utku/playground/justhink-dialogue-and-actions-corpus/processed_data/dialign_outputs\n",
      "Running for dialogues...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "cmd = 'java -jar {} -i {} -o {}'.format(\n",
    "    dialign_jar_file.resolve(), \n",
    "    dialign_inputs_dir.resolve(), \n",
    "    dialign_outputs_dir.resolve())\n",
    "print(cmd)\n",
    "\n",
    "print('Running for dialogues...')\n",
    "!$cmd\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read routine tables.\n",
    "i.e. shared expression lexicons as termed by dialign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read for team 07: 384 routines\n",
      "Read for team 08: 420 routines\n",
      "Read for team 09: 533 routines\n",
      "Read for team 10: 226 routines\n",
      "Read for team 11: 371 routines\n",
      "Read for team 17: 149 routines\n",
      "Read for team 18: 149 routines\n",
      "Read for team 20: 287 routines\n",
      "Read for team 28: 194 routines\n",
      "Read for team 47: 223 routines\n"
     ]
    }
   ],
   "source": [
    "routine_dfs = dict()\n",
    "\n",
    "for team_no in sorted(transcript_dfs):\n",
    "    routine_file = 'justhink19_dialogue_{:02d}_tsv-lexicon.tsv'.format(\n",
    "        team_no)\n",
    "    routine_file = dialign_outputs_dir.joinpath(routine_file)\n",
    "    df = pd.read_csv(str(routine_file), sep='\\t')\n",
    "    print('Read for team {:02d}: {} routines'.format(team_no, len(df)))\n",
    "\n",
    "    l = list()\n",
    "    for e in df['Surface Form']:\n",
    "        tokenized_e = [t.text for t in tokenizer(e)]\n",
    "        v = 0\n",
    "        for n in task_words:\n",
    "            # if n in e:\n",
    "            if n in tokenized_e:\n",
    "                v += 1\n",
    "        l.append(v)\n",
    "    df.insert(3, 'task_spec_referent_count', l)\n",
    "\n",
    "    df['utterance_no_list'] = [[int(n) for n in seq.split(', ')]\n",
    "                       for seq in df['Turns']]\n",
    "\n",
    "\n",
    "    routine_dfs[team_no] = df\n",
    "\n",
    "# # Example/debugging.\n",
    "# team_no = 18\n",
    "# routine_dfs[team_no].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter for routines with task-specific referents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for team_no, df in routine_dfs.items():\n",
    "    df = df[df.task_spec_referent_count > 0]\n",
    "    df = df.drop('task_spec_referent_count', axis=1)\n",
    "    routine_dfs[team_no] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export routine tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting routine tables...\n",
      "Exported routines for  7 to ../processed_data/routines/justhink19_routines_07.csv\n",
      "Exported routines for  8 to ../processed_data/routines/justhink19_routines_08.csv\n",
      "Exported routines for  9 to ../processed_data/routines/justhink19_routines_09.csv\n",
      "Exported routines for 10 to ../processed_data/routines/justhink19_routines_10.csv\n",
      "Exported routines for 11 to ../processed_data/routines/justhink19_routines_11.csv\n",
      "Exported routines for 17 to ../processed_data/routines/justhink19_routines_17.csv\n",
      "Exported routines for 18 to ../processed_data/routines/justhink19_routines_18.csv\n",
      "Exported routines for 20 to ../processed_data/routines/justhink19_routines_20.csv\n",
      "Exported routines for 28 to ../processed_data/routines/justhink19_routines_28.csv\n",
      "Exported routines for 47 to ../processed_data/routines/justhink19_routines_47.csv\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Exporting routine tables...')\n",
    "\n",
    "for team_no, df in routine_dfs.items():\n",
    "    # Construct filename.\n",
    "    file = 'justhink19_routines_{:02d}.csv'.format(team_no)\n",
    "    file = routines_dir.joinpath(file)\n",
    "\n",
    "    # Write the table to file.\n",
    "    df.to_csv(file, index=False, sep='\\t')\n",
    "\n",
    "    print('Exported routines for {:2d} to {}'.format(team_no, file))\n",
    "    \n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
